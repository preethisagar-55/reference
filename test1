consolidated scan jenkins file concept
----------------------------------------

checkmarx - security tool -> detects vulnerabilities in source code (SAST).
sonarqube - Code quality ‚Äì detects bugs, code smells, maintainability issues, and some security flaws.

scansRuns Checkmarx One SAST scan on the source code.

Runs container image security scan (Sysdig).

For release builds only, it consolidates scan results.

fileCreate ‚Üí generates a consolidated scan file (merging SAST + container scan results).

fileUpload ‚Üí uploads the consolidated file to a central location (artifact repo or dashboard).



Checks out multiple repos.

Runs Checkmarx One SAST scans.

Runs container security scans.

Consolidates results into files (for release builds).

Uploads consolidated scan artifacts for governance/compliance.

Handles errors and notifications gracefully.




‚ÄúWhether the Checkmarx scan stage runs or not is controlled by the cxScan flag inside the branch‚Äëspecific YAML file. In release.yaml it is set to true, so the pipeline shows a stage called Consolidated scans and triggers the Checkmarx‚ÄëScan job. In develop.yaml it is set to false, so that stage is skipped and you don‚Äôt see it in the logs.‚Äù

That‚Äôs the essence:

Stage name = ‚ÄúConsolidated scans‚Äù

Actual job = Checkmarx‚ÄëScan

Flag source = YAML (release.yaml vs develop.yaml)



--->signum container sigin

Signs the Docker image and verifies the signature.

Ensures image integrity and authenticity before publishing.
----------------------------------------------------------------
This Jenkins pipeline uses shared libraries to centralize configuration and stage logic, loads branch-specific stage flags from YAML via global.getStages, and runs on Kubernetes pod templates with multiple tool containers (jnlp, maven, helm, occlient, signum). It checks out code, detects which microservices changed, and builds the project to produce artifact metadata like versionNumber. If enabled by the YAML (e.g., sonar: true), it performs SonarQube analysis in the maven container using a token and branch context; if cxScan is true (release branches), it triggers a downstream Checkmarx-Scan job inside a stage named ‚ÄúConsolidated scans.‚Äù For dockerBuild, it acquires a lock to avoid concurrent contention, determines the target Docker registry (release vs snapshot), then for each changed service in parallel it copies a common Dockerfile and Helm scaffolding, updates Chart.yaml, installs the build chart, and uses the occlient to run oc start-build with the service directory as context so OpenShift builds and pushes the image; afterward it signs and verifies the image with Signum. The build-publish-charts stage packages Helm charts for both core (helm/) and AWS (helm-aws/) paths, uploads them to Artifactory under the appropriate repository (release/snapshot, with aws subpath where needed), and records published chart versions. When deploy: true (e.g., develop), it runs two deployments via a shared deploy function‚Äîone to Dev and another to Dev-ETIN‚Äîpassing identifiers and the built version. Finally, it sets the build status and always sends a Microsoft Teams notification, with milestones to gate progress and parallel failFast to stop quickly on errors.

-------------------------------------------------------------------

stage wise explanation:
Stage‚Äëwise Explanation
1. Checkout
Runs in the jnlp container.

Checks out source code from SCM (Bitbucket).

Captures commit ID (scmVars.GIT_COMMIT) for traceability.

2. Getting Changed Services
Still in jnlp.

Uses regex to detect which microservices changed (core-* or infra-*).

Stores the list in services for later build/deploy.

Logs the services for debugging.

3. Build Project
Calls a shared library function buildProject.

Produces artifact metadata: artifactName, versionNumber, etc.

This version number is reused in later stages (charts, docker images, deployments).

4. Consolidated Scans (Checkmarx)
Conditional: runs only if globalStages.cxScan = true (e.g., release branch).

Stage name: "Consolidated scans".

Triggers a downstream Jenkins job: Scans/Checkmarx-Scan/....

Passes versionNumber as parameter.

Purpose: static security analysis.

5. Sonar Scan
Conditional: runs if globalStages.sonar = true.

Runs in maven container.

Uses SonarQube environment (SQ3) and Jenkins credentials for token.

Executes Maven Sonar plugin:

Code
mvn sonar:sonar -Dsonar.host.url=... -Dsonar.branch.name=... -Dsonar.login=$token
Purpose: code quality analysis.

6. Docker Build
Conditional: runs if globalStages.dockerBuild = true.

Protected by lock('rcs-ci') to avoid concurrent builds.

Stage name: "docker-build".

For each changed service (in parallel):

Copy common Dockerfile and Helm chart templates into service directory.

Update Chart.yaml  with service name and version.

Run Helm install of build chart.

Use occlient to run oc start-build in OpenShift, building the Docker image from service directory.

Verify build success.

Sign and verify the image with Signum for integrity.

Purpose: build and push Docker images into registry (release or snapshot).

7. Build & Publish Charts
Runs in helm container.

Two directories processed:

helm/ ‚Üí core services.

helm-aws/ ‚Üí AWS services + prerequisites.

For each service:

Package Helm chart (enaHelm.packageChart).

Fetch chart name and version.

Upload .tgz chart file to Artifactory (release or snapshot repo).

Record chart in publishedCharts.

Runs builds in parallel with failFast.

Purpose: publish Helm charts so they can be deployed.

8. Deploy‚ÄëDev
Conditional: runs if globalStages.deploy = true (e.g., develop branch).

Calls shared deploy function with config, version, identifiers.

Deploys services into Dev environment using Helm charts.

9. Deploy‚ÄëDevEtin
Additional deployment stage.

Same as above but with different trigger identifier (webhooktriggerdevetin).

Deploys into Dev‚ÄëETIN environment (integration testing cluster).

10. Error Handling & Notifications
Catch block: marks build as Failure if any stage errors.

Finally block:

If no result set, mark as SUCCESS.

Always notify Microsoft Teams via webhook with build status.

‚úÖ End‚Äëto‚ÄëEnd Flow
Checkout ‚Üí Detect Changed Services ‚Üí Build Project ‚Üí (Checkmarx Scan if release) ‚Üí Sonar Scan ‚Üí Docker Build ‚Üí Publish Helm Charts ‚Üí Deploy to Dev & DevEtin ‚Üí Notify Teams
-----------------------

üîë Additional Key Points
Shared Libraries:
The pipeline heavily relies on Jenkins shared libraries (common-lib, retirement-cicd-library). This makes the pipeline modular, reusable, and easier to maintain. Functions like global.getConfig, global.getStages, enaMaven.executeCmdLine, enaHelm.packageChart, and deploy all come from these libraries.

Branch‚Äëbased Behavior:
The pipeline dynamically changes which stages run depending on branch type (develop.yaml, release.yaml, etc.). For example, cxScan is false in develop but true in release, so Checkmarx runs only for release builds. This is a common CI/CD pattern to enforce stricter checks on release branches.

Parallelization:
Both Docker builds and Helm chart packaging run in parallel across services, with failFast = true. This speeds up builds and ensures that if one service fails, the pipeline stops quickly.

Resource Isolation:
Each stage runs inside a specific container (maven, helm, occlient, signum) defined in the Kubernetes pod template. This isolates tools, ensures consistent environments, and avoids dependency conflicts.

Locks & Milestones:

lock('rcs-ci') prevents multiple pipelines from building Docker images simultaneously, avoiding race conditions.

milestone() ensures that older builds don‚Äôt overtake newer ones in multi‚Äëbranch pipelines.

Security & Compliance:

Secrets are injected via Jenkins credentials (withCredentials).

Docker images are signed and verified with Signum before publishing.

Scans (SonarQube, Checkmarx, Sysdig, SEI) enforce code quality and security compliance.

Artifact Management:
Helm charts and Docker images are uploaded to Artifactory (release or snapshot repos). This provides versioned, traceable artifacts for deployment.

Deployment Strategy:
Deployments are automated via Helm into multiple environments (Dev, Dev‚ÄëETIN). The pipeline uses identifiers (projectIdentifier, pipelineIdentifier, triggerIdentifier) to track and trigger deployments consistently.

Notifications:
Build results are always sent to Microsoft Teams via webhook, ensuring visibility for the team.

Error Handling:
The pipeline uses try/catch/finally to mark builds as failed on errors, but still guarantees notifications are sent.

---------------------------------------------------------------------------------------------------------------------

This Jenkins pipeline is built with shared libraries to keep the logic modular and reusable. It starts by checking out code and detecting which microservices changed, then builds the project and determines versioning. Depending on branch type, it conditionally runs security scans like Checkmarx and quality analysis with SonarQube. It then builds Docker images in OpenShift, signs them for integrity, and packages Helm charts which are uploaded to Artifactory. Finally, it deploys services into Dev and Dev‚ÄëETIN environments when enabled, and always sends build status notifications to Microsoft Teams. The pipeline uses Kubernetes pod templates for tool isolation, parallel execution for efficiency, and branch‚Äëspecific YAML configs to control which stages run, making it a robust CI/CD setup for microservices.‚Äù

This is concise, covers all stages, and highlights best practices ‚Äî perfect for interviews.

-------------------------------------CD------------------------

A dry run ensures manifests are valid, images are correct, and no surprises occur before actual deployment.

It acts as a quality gate for production-ready code.

This Harness CD template defines a complete deployment stage using Native Helm. When triggered, it first performs a Kubernetes dry run for release branches to validate manifests and extract container image references. A shell script then parses those manifests to collect the images and, if needed, rewrites them from a development registry to a release registry to ensure proper artifact promotion. Next, a container verification step group runs security or compliance checks on those images. Once validation passes, the HelmDeploy step applies the actual Helm release to the target environment and infrastructure, which are chosen dynamically based on pipeline variables. The template supports deploying multiple services in parallel, retries failed deployments up to three times, and automatically rolls back if errors persist. Delegate selection is also dynamic, routing work to the right Harness delegate depending on the environment (such as performance, qualification, or UAT). In short, this CD pipeline ensures safe, validated, and secure deployments by combining pre-deployment checks, image promotion, container verification, Helm-based rollout, and robust failure handling, with special safeguards applied to release branches.

----------------------------------------------------------------------
latest version retirement logic explanation

Initialization

Sets variables like project, branch, infra, trigger type, and credentials.

Ensures jq is available for JSON parsing.

Branch & Version Validation

If branch is a release branch (release/YYYY.N.N):

majorVersion must match the branch version.

Repo = project-helm-release-local.

If branch is develop/feature:

majorVersion must be empty.

Repo = project-helm-snapshot-local.

Infra Validation

Maps infra values to prod or nonprod.

Errors out if infra is invalid.

AQL Query Construction

Manual trigger + version given ‚Üí check if that exact artifact exists.

Auto trigger + version given ‚Üí find latest patch for that major version.

No version given ‚Üí fetch all artifacts and pick the latest.

Fetch & Process Results

Sends query to Artifactory API.

Uses jq to parse results.

Either confirms version exists or selects the latest one.

Prints services included in that chart version.

Error Handling

If rules are violated (wrong version, invalid infra, missing artifact), script exits with a clear error message.
------------------------------------------
ssh key notes


Generate Private Key  
openssl genrsa -aes256 -out private.key  
‚Üí Enter and remember passphrase.

Export Public Key  
ssh-keygen -y -f private.key  
‚Üí Copy output (starts with ssh-rsa) to server.

Server Setup

Switch to target user: dzdo su - <user>

Create .ssh directory: mkdir -p ~/.ssh

Go to .ssh: cd ~/.ssh

Append public key to authorized_keys file.

Set permissions: chmod 600 authorized_keys.

Test Authentication  
ssh -v -i private.key <user>@<server>  
‚Üí Enter passphrase when prompted.
‚Üí Successful login shows $ prompt.

Troubleshooting

Use -v for verbose output.

Check sshd_config for proper pubkey settings.

-------------------------------------------------notes------------------

curl -u <+pipeline.variables.ARTIFACTORY_USERNAME>:<+pipeline.variables.ARTIFACTORY_PASSWORD> -XGET https://artifactory/rcs-helm-snapshot-local/core-annuities-service-2024.4.0-SNAPSHOT.tgz -T  core-annuities-service-2024.4.0-SNAPSHOT.tgz

curl -u <+pipeline.variables.ARTIFACTORY_USERNAME>:<+pipeline.variables.ARTIFACTORY_PASSWORD> -XGET https://artifactory/rcs-helm-snapshot-local/core-annuities-service-2024.4.0-SNAPSHOT.tgz -o core-annuities-service-2024.4.0-SNAPSHOT.tgz
-T ‚Üí Upload (send local file ‚Üí server)

-o ‚Üí Download (save server response ‚Üí local file)
capital O -> if you want curl to keep the original filename from the URL automatically.

curl -u "{artifactory_username}:$artifactory_password" -O https://artifactory/rcore-maven-snapshot-local/sample/2024.2.0-SNAPSHOT/sample-core-2024.2.0-20240403.160130-1.tar.gz


curl -x proxy.prod.local:8080 -u 'Svc-RetirementDevOps:<+secrets.getValue("org.rngartifactorysecret")>' \
-kv "https://artifactory/artifactory/rcore-maven-snapshot-local/com/fis/wra/retirement/retirement-core-moddev/<+pipeline.variables.Artifact_Version>/$latest_file" \
-o "$latest_file" -1

tar -xvf core-annuities-service-2024.4.0-SNAPSHOT.tgz

 Go to your target namespace and select User Management > RoleBindings
Create a Binding
Name: Any name you want, typically the service account name (in some form) and the role being granted
Namespace: The target namespace
Role name: Edit
Subject: ServiceAccount
Subject namespace: The namespace where the service account was created
Subject name: retirement-devops-jenkins-cicd.

-----------------------------------------------------------------------------------------------------
export $NAMESPACE= "<+stage.variables.namespace>"
POD_NAME=$(kubectl get pods -n retire-sandbox-dev -l app=infra-configuration-service  -o jsonpath="{.items[0].metadata.name}")
POD_STATUS=$(kubectl get pods $POD_NAME -n retire-sandbox-dev -o jsonpath="{.status.phase}")
check_pod_status(){
    #pod_name=$1
    #namespace=$2
    status=$(kubectl get pods "$POD_NAME" -n "$NAMESPACE" --no-headers | awk '{print$3}')
    if ["$POD_STATUS"=="Running"];then
     echo "pod $POD_NAME is running."
    else
     echo "pod $POD_NAME is not yet running.waiting..."
    fi
}
#while! check_pod_status
#"$expected_pod" "$namespace";
#do sleep 10

#echo "pod $expected_pod is up and running.proceeding with further deployment"
-------------------------------------------------------------------------harness delegate------------------------------------------------------------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: harness-performance-account-token
  namespace: retirement-perf
  labels:
    delegate: harness-performance-delegate
type: Opaque
data:
  DELEGATE_TOKEN: "blablabla"
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: perf-delegate
  namespace: retirement-perf
  labels:
    harness.io/name: perf-delegate
spec:
  replicas: 1
  selector:
    matchLabels:
      harness.io/name: delegate
  template:
    metadata:
      labels:
        harness.io/name: delegate
      annotations:
        prometheus.io/path: /api/metrics
        prometheus.io/port: '3460'
        prometheus.io/scrape: 'true'
    spec:
      restartPolicy: Always
      serviceAccountName: retirement-performance-sa
      imagePullSecrets:
        - name: rcs-artifactory
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 600
      containers:
        - name: delegate
          image: 'rcs-docker-dev.docker/harness/delegate:24.04.82804'
          imagePullPolicy: Always
          resources:
            limits:
              cpu: '1'
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 2Gi
          ports:
            - containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /api/health
              port: 3460
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
            periodSeconds: 60
            successThreshold: 1
            failureThreshold: 5
          startupProbe:
            httpGet:
              path: /api/health
              port: 3460
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 15
          env:
            - name: JAVA_OPTS
              value: '-Xms64M'
            - name: ACCOUNT_ID
              value: XzgUIuZXRMWymCzLKYZpCg
            - name: MANAGER_HOST_AND_PORT
              value: 'https://app.harness.io/gratis'
            - name: DEPLOY_MODE
              value: KUBERNETES
            - name: DELEGATE_NAME
              value: harness-performance-delegate
            - name: DELEGATE_TYPE
              value: KUBERNETES
            - name: DELEGATE_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: NEXT_GEN
              value: 'true'
            - name: CLIENT_TOOLS_DOWNLOAD_DISABLED
              value: 'true'
            - name: LOG_STREAMING_SERVICE_URL
              value: 'https://app.harness.io/gratis/log-service/'
            - name: DYNAMIC_REQUEST_HANDLING
              value: 'false'
            - name: PROXY_HOST
              value: proxy.prod.local
            - name: PROXY_PORT
              value: '8080'
            - name: PROXY_MANAGER
              value: 'true'
            - name: NO_PROXY
              value: prod.local
            - name: HTTPS_PROXY
              value: 'http://proxy.prod.local:8080'
          envFrom:
            - secretRef:
                name: harness-performance-account-token
          securityContext:
            allowPrivilegeEscalation: false
      serviceAccount: retirement-performance-sa
      dnsPolicy: ClusterFirst
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600





